#!/usr/bin/env python3
"""
Clean / rewrite BLIP captions using VG scene graphs BEFORE VGâ†’LAION conversion.
"""

import argparse
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Tuple

import h5py

from openai import OpenAI



SYSTEM_PROMPT = """You are a caption rewriter that MUST stay grounded in the given scene graph.

You will receive:
- A RAW_CAPTION: a noisy caption generated by an image captioning model.
- A list of ALLOWED_OBJECTS: object categories that are actually present.
- A list of REGIONAL_DESCRIPTIONS: short phrases describing local regions
  of the image in terms of those objects and their relations.

Your task:
- Produce a SINGLE, global caption that:
  - Only mentions objects from ALLOWED_OBJECTS (no new object categories).
  - Is consistent with REGIONAL_DESCRIPTIONS as much as possible.
  - Is natural, fluent English, and works as a good image generation prompt.
- You may use synonyms/phrasing changes, but do NOT add new object types
  or obviously impossible content.
- You may add plausible attributes (colors, materials, etc.) that match
  the REGIONAL_DESCRIPTIONS or are generic (e.g., "a clear blue sky"),
  but avoid inventing completely unrelated scenes.

Output rules:
- Return ONLY the final cleaned caption text, no JSON, no extra commentary.
"""


def _decode_path(value) -> str:
    if isinstance(value, bytes):
        return value.decode("utf-8")
    if hasattr(value, "tobytes"):
        return value.tobytes().decode("utf-8")
    return str(value)


def build_sg_index(
    dataset_dir: Path,
    splits: List[str],
    vocab_json: Path,
) -> Dict[str, Dict[str, Any]]:
    """
    Build a mapping from image_path -> {items, relations} using VG HDF5 + vocab.

    - items: [{"item_id", "label", "attributes": []}, ...]
    - relations: [{"item1", "item2", "relation"}, ...]
    """
    if not vocab_json.exists():
        raise FileNotFoundError(f"Missing vocab file: {vocab_json}")
    vocab = json.loads(vocab_json.read_text())
    obj_names = vocab["object_idx_to_name"]
    pred_names = vocab["pred_idx_to_name"]

    sg_index: Dict[str, Dict[str, Any]] = {}

    for split in splits:
        h5_path = dataset_dir / f"{split}.h5"
        if not h5_path.exists():
            continue
        with h5py.File(h5_path, "r") as f:
            image_paths = f["image_paths"][:]
            object_names = f["object_names"][:]
            objects_per_image = f["objects_per_image"][:]
            rel_subjects = f["relationship_subjects"][:]
            rel_objects = f["relationship_objects"][:]
            rel_predicates = f["relationship_predicates"][:]
            relationships_per_img = f["relationships_per_image"][:]

            total = len(image_paths)
            for idx in range(total):
                rel_path = _decode_path(image_paths[idx])
                if rel_path in sg_index:
                    continue

                num_obj = int(objects_per_image[idx])
                num_rel = int(relationships_per_img[idx])
                if num_obj <= 0:
                    continue

                items = []
                slot_to_item = {}

                # objects
                for slot in range(num_obj):
                    name_idx = int(object_names[idx, slot])
                    if name_idx < 0 or name_idx >= len(obj_names):
                        continue
                    label = obj_names[name_idx]
                    item = {
                        "item_id": len(items),
                        "label": label,
                        "attributes": [],  # we ignore attrs (cause VG tpically has it empty) here for simplicity
                    }
                    slot_to_item[slot] = item["item_id"]
                    items.append(item)

                if not items:
                    continue

                # relations
                relations = []
                for r in range(num_rel):
                    sub_slot = int(rel_subjects[idx, r])
                    obj_slot = int(rel_objects[idx, r])
                    pred_idx = int(rel_predicates[idx, r])
                    if (
                        sub_slot not in slot_to_item
                        or obj_slot not in slot_to_item
                        or pred_idx < 0
                        or pred_idx >= len(pred_names)
                    ):
                        continue
                    relations.append(
                        {
                            "item1": slot_to_item[sub_slot],
                            "item2": slot_to_item[obj_slot],
                            "relation": pred_names[pred_idx],
                        }
                    )

                sg_index[rel_path] = {
                    "items": items,
                    "relations": relations,
                }

    return sg_index


def build_regional_descriptions(items: List[Dict[str, Any]], relations: List[Dict[str, Any]]) -> List[str]:
    """Heuristically build regional descriptions from items + relations."""
    items_map = {it["item_id"]: it for it in items}
    regions: List[str] = []

    def _obj_phrase(item_id: int) -> str:
        it = items_map[item_id]
        return it["label"]

    for rel in relations:
        i1 = rel["item1"]
        i2 = rel["item2"]
        if i1 not in items_map or i2 not in items_map:
            continue
        subj = _obj_phrase(i1)
        obj = _obj_phrase(i2)
        pred = rel["relation"]
        regions.append(f"{subj} {pred} {obj}")

    seen = set()
    uniq_regions = []
    for r in regions:
        if r not in seen:
            seen.add(r)
            uniq_regions.append(r)
    return uniq_regions


def build_user_prompt(raw_caption: str, items: List[Dict[str, Any]], relations: List[Dict[str, Any]]) -> str:
    allowed_objects = sorted({it["label"] for it in items})
    regions = build_regional_descriptions(items, relations)

    parts: List[str] = []
    parts.append("RAW_CAPTION:")
    parts.append(raw_caption.strip())
    parts.append("\nALLOWED_OBJECTS (only these object categories may be mentioned):")
    parts.append(", ".join(allowed_objects))

    if regions:
        parts.append("\nREGIONAL_DESCRIPTIONS (hints, may be partial):")
        for r in regions[:12]:
            parts.append(f"- {r}")
    else:
        parts.append("\nREGIONAL_DESCRIPTIONS: (none available) ")

    parts.append(
        "\nNow rewrite the RAW_CAPTION into a single, globally consistent caption "
        "that obeys the constraints above."
    )
    return "\n".join(parts)


def call_llm(prompt: str, model: str, temperature: float) -> str:
    api_key = os.getenv("OPENAI_API_KEY")

    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=temperature,
    )
    text = resp.choices[0].message.content or ""
    return text.strip()


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Clean BLIP captions using VG scene graphs (pre-conversion).")
    p.add_argument("--dataset_dir", type=Path, default=Path("datasets/vg"),
                   help="Directory containing {train,val,test}.h5")
    p.add_argument("--splits", nargs="+", default=["train", "val", "test"],
                   help="Splits to use for SG info.")
    p.add_argument("--vocab_json", type=Path, default=Path("datasets/vg/vocab.json"),
                   help="Path to VG vocab.json.")
    p.add_argument("--captions_in", type=Path, default=Path("datasets/vg/captions.json"),
                   help="Input BLIP captions.json.")
    p.add_argument("--captions_out", type=Path, default=Path("datasets/vg/captions_clean.json"),
                   help="Output cleaned captions.json.")
    p.add_argument("--num_samples", type=int, default=None,
                   help="Limit number of captions to clean (for debugging).")
    p.add_argument("--model", type=str, default="gpt-4o-mini",
                   help="OpenAI model name.")
    p.add_argument("--temperature", type=float, default=0.3,
                   help="Sampling temperature.")
    return p.parse_args()


def main() -> None:
    args = parse_args()

    # Build SG index: image_path -> {items, relations}
    print("Building scene graph index from HDF5 + vocab...")
    sg_index = build_sg_index(args.dataset_dir, args.splits, args.vocab_json)
    print(f"SG index has {len(sg_index)} images.")

    # Load captions.json
    caps_data = json.loads(args.captions_in.read_text())
    entries = caps_data.get("captions", [])
    if args.num_samples is not None:
        entries = entries[: args.num_samples]

    cleaned_entries: List[Dict[str, str]] = []

    for i, entry in enumerate(entries):
        rel_path = entry["image_path"]
        raw_caption = entry["caption"]
        sg = sg_index.get(rel_path)
        if sg is None:
            # No SG info so keep original caption edge cases
            cleaned_entries.append(entry)
            continue

        user_prompt = build_user_prompt(raw_caption, sg["items"], sg["relations"])
        try:
            new_caption = call_llm(user_prompt, model=args.model, temperature=args.temperature)
        except Exception as e:  # pragma: no cover
            print(f"WARNING: LLM caption cleaning failed for {rel_path}: {e}")
            cleaned_entries.append(entry)
            continue

        cleaned_entries.append({"image_path": rel_path, "caption": new_caption})

        if (i + 1) % 10 == 0:
            print(f"Processed {i + 1} captions...")



    out_data = {"captions": cleaned_entries}
    args.captions_out.parent.mkdir(parents=True, exist_ok=True)
    args.captions_out.write_text(json.dumps(out_data, indent=2))
    print(f"Saved {len(cleaned_entries)} cleaned captions to {args.captions_out}")


if __name__ == "__main__":
    main()